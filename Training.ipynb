{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib                                                              #saving the fitted svm model/classifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailspam=[]\n",
    "directory=\"data/spam\"                                                       #reading the spam emails\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    f=open(directory+\"/\"+file,\"r\",encoding='cp437')\n",
    "    b=f.readline().lower()\n",
    "    b=b.replace(\"subject: fw :\",\"\")\n",
    "    b=b.replace(\"subject: fw : fw :\",\"\")\n",
    "    b=b.replace(\"subject: re : re :\",\"\")\n",
    "    b=b.replace(\"subject: re :\",\"\")\n",
    "    b=b.replace(\"subject: \",\"\")\n",
    "    a=b+f.read().lower()\n",
    "    emailspam.append(a)\n",
    "    \n",
    "emailham=[]\n",
    "directory=\"data/ham\"                                                        #reading the ham emails\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    f=open(directory+\"/\"+file,\"r\",encoding='cp437')\n",
    "    a=f.readline().lower()\n",
    "    start=1\n",
    "    b=\"\"\n",
    "    while a!=\"\":\n",
    "        if start==1:\n",
    "            b+=a\n",
    "            a=f.readline()\n",
    "        else:\n",
    "            a=f.readline()\n",
    "            \n",
    "        if a[:7]==\"subject\":\n",
    "            start=1\n",
    "        elif a[:10]==\"- - - - - \" or a==\"\":\n",
    "            start=0\n",
    "            emailham.append(b)\n",
    "            b=\"\"\n",
    "\n",
    "            \n",
    "for i in range(len(emailham)):\n",
    "    b=emailham[i]\n",
    "    b=b.replace(\"subject: fw :\",\"\")\n",
    "    b=b.replace(\"subject: fw : fw :\",\"\")\n",
    "    b=b.replace(\"subject: re : re :\",\"\")\n",
    "    b=b.replace(\"subject: re :\",\"\")\n",
    "    b=b.replace(\"subject: \",\"\")\n",
    "    emailham[i]=b\n",
    "    \n",
    "s_len=len(emailspam)\n",
    "h_len=len(emailham)\n",
    "emails=emailspam+emailham\n",
    "labels=np.hstack(((np.ones(s_len),np.zeros(h_len))))                           \n",
    "emails=np.vstack((np.array(emails).transpose(),labels)).transpose()     #creating a list of emails with corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_color_removal(email):                      #function to remove punctuation marks and  hex color codes\n",
    "    e_mod=email\n",
    "    sym=[\"\\\\\",\"&\",\"!\",\".\",\",\",\"/\",\"+\",\"*\",\"%\",\"(\",\")\",\":\",\";\",\"\\\"\",\"_\",\"?\",\"=\",\"@\",\"{\",\"}\",\"[\",\"]\",\"<\",\">\",\"|\"]\n",
    "    \n",
    "    f=open(\"data/color_codes.txt\",\"r\")\n",
    "    color=f.read().split(\"\\n\")\n",
    "    for s in sym:\n",
    "        e_mod=e_mod.replace(s,\" \")\n",
    "    e_mod=e_mod.replace(\"\\'\",\"\")\n",
    "    e_mod=e_mod.replace(\"\\n\",\" \")\n",
    "    e_mod=e_mod.replace(\"-\",\"\")\n",
    "    for c in color:\n",
    "        e_mod=e_mod.replace(c,\"\")\n",
    "        \n",
    "    return e_mod\n",
    "\n",
    "def lemma_dict():   #creating a python dictionary with keys as words and values as the corresponding root words(lemma)  \n",
    "    l_dic={}\n",
    "    f=open(\"data/lemma.txt\",\"r\")\n",
    "    for i in range(20000):\n",
    "        a=f.readline()\n",
    "        b=a.replace(\"\\n\",\"\")\n",
    "        c=b.strip().split(\"->\")\n",
    "        d=c[1].strip().split(\",\")\n",
    "        for ele in d:\n",
    "            l_dic[ele]=c[0]\n",
    "            \n",
    "    return l_dic\n",
    "\n",
    "def lemmatisation(email,dic):   #function to lemmatise the emails using the lemma dictionary( generated via lemma_dict())\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        e_mod=email.replace(key,\" \"+dic[key]+\" \")\n",
    "    \n",
    "    return e_mod\n",
    "\n",
    "\n",
    "def num_token(email):                              #function to remove the digits\n",
    "    num=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    e_mod=email\n",
    "    for n in num:\n",
    "        e_mod=e_mod.replace(n,\" \")\n",
    "    \n",
    "    return e_mod\n",
    "\n",
    "def stop_words():                              #reading the English stop_words file\n",
    "    f=open(\"data/stop_words.txt\",\"r\")\n",
    "    stop_words=f.read().split(\"\\n\")\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "def html_words():\n",
    "    f=open(\"data/html_words.txt\",\"r\")          #reading the html_words file\n",
    "    h_words=f.read().split(\"\\n\")\n",
    "\n",
    "    return h_words\n",
    "\n",
    "def rtf_words():                              #reading the rtf_words file\n",
    "    f=open(\"data/rtf_words.txt\",\"r\")\n",
    "    r_words=f.read().split(\"\\n\")\n",
    "\n",
    "    return r_words\n",
    "\n",
    "def idf(bag,lis):                             #function to compute the idf vector for the emails/documents\n",
    "    idf_list=[]\n",
    "    N=len(lis)\n",
    "    for term in bag:\n",
    "        a=0\n",
    "        for doc in lis:\n",
    "            if term in doc:\n",
    "                a+=1\n",
    "        idf_list.append(a)\n",
    "    idf_vector=np.log(N/np.array(idf_list,dtype=\"float64\"))\n",
    "    return idf_vector\n",
    "\n",
    "def tf(bag,doc):\n",
    "    N=len(bag)\n",
    "    tf_list=[]                                  #func to compute the term frequency vector for each email\n",
    "    for i in range(N):\n",
    "        count=doc.count(bag[i])\n",
    "        tf_list.append(count/len(doc))\n",
    "    tf_vector=np.array(tf_list,dtype=\"float64\")\n",
    "    return tf_vector\n",
    "\n",
    "def tf_idf_vectorisation(bag,emails):\n",
    "    idf_v=idf(bag,emails)\n",
    "    N=len(bag)\n",
    "    num_of_docs=len(emails)\n",
    "    X=np.empty((N,num_of_docs))                #converts the list of emails into corresponding tf-idf vector array\n",
    "    for i in range(num_of_docs):\n",
    "        tf_v=tf(bag,emails[i])\n",
    "        X[:,i]=tf_v*idf_v      \n",
    "    \n",
    "    return X\n",
    "\n",
    "def smote(X,N,k=5):\n",
    "    X_mod=X.transpose()                     # SMOTE for balancing the minority class(spams) \n",
    "    for i in range(X.shape[1]):\n",
    "        current=X[:,i]\n",
    "        dist=np.linalg.norm(X.transpose()-X[:,i],axis=1)\n",
    "        dist_list=dist.tolist()\n",
    "        neighbours=[]\n",
    "        for j in range(k):\n",
    "            min_pos=dist_list.index(min(dist_list))\n",
    "            neighbours.append(X[:,min_pos])\n",
    "            dist_list[min_pos]=max(dist_list)+1000\n",
    "\n",
    "        a=np.arange(k)\n",
    "        np.random.shuffle(a)\n",
    "        neighbours_array=(np.array(neighbours))[a[:N]]\n",
    "        c=np.random.uniform(0.0001,1,N)\n",
    "        new_x=current+(c*(neighbours_array-current).transpose()).transpose()\n",
    "        X_mod=np.vstack((X_mod,new_x))\n",
    "        \n",
    "    return (X_mod.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_classifier.sav']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_dictionary=lemma_dict()                       # generating the lemma dictionary\n",
    "for i in range(len(emails)):\n",
    "    emails[i,0]=symbol_color_removal(emails[i,0])       \n",
    "    emails[i,0]=num_token(emails[i,0])\n",
    "    emails[i,0]=lemmatisation(emails[i,0],lemma_dictionary)\n",
    "    \n",
    "s_words=stop_words()\n",
    "h_words=html_words()\n",
    "r_words=rtf_words()\n",
    "length=len(emails)                   #processing such that each email is converted into list of words(filtered/parsed)\n",
    "emails_list=[]\n",
    "emails_list_labels=[]\n",
    "for i in range(length):\n",
    "    e=emails[i,0].strip().split(\" \")   \n",
    "    l=[x for x in e if x not in s_words and  x not in h_words and x not in r_words and x!=\" \" and x!=\"\" and x!=\"#\"]\n",
    "    if l!=[]:\n",
    "        emails_list.append(l)\n",
    "        emails_list_labels.append(emails[i,1])                  \n",
    "        \n",
    "emails_list_flat=[i for j in emails_list for i in j]\n",
    "emails_bag=list(set(emails_list_flat))      #creating the bag_of_words(unique words across the documents/list of emails)\n",
    "\n",
    "X_nolabel=tf_idf_vectorisation(emails_bag,emails_list)   # performing tf-idf\n",
    "\n",
    "\n",
    "pos=list(map(float,emails_list_labels)).index(0.0)\n",
    "no_of_emails=len(emails_list_labels)\n",
    "no_of_spams=pos\n",
    "no_of_hams=no_of_emails-no_of_spams\n",
    "rate=int(no_of_hams/no_of_spams) -1         #determining how many neighbours to consider per email or feature vector\n",
    "\n",
    "spam_balanced=smote(X_nolabel[:,:no_of_spams],rate)     #performing smote\n",
    "\n",
    "X=np.vstack((spam_balanced.transpose(),X_nolabel[:,no_of_spams:].transpose())).transpose()\n",
    "spam_count_balanced=spam_balanced.shape[1]\n",
    "y=np.hstack((np.ones(spam_count_balanced),np.zeros(no_of_hams)))\n",
    "\n",
    "np.save(\"X.npy\",X)               #saving the (feature extracted and balanced) dataset\n",
    "np.save(\"y.npy\",y)               #saving the corresponding labels\n",
    "np.save(\"lemma_dictionary.npy\",lemma_dictionary) # saving the lemma_dictionary generated(loaded in \"prediction.ipynb\" to lemmatise the test emails)\n",
    "np.save(\"bagofwords.npy\",emails_bag)        #saving the bag_of_words list(loaded in \"prediction.ipynb\" for tf-idf vectorisation)\n",
    "\n",
    "lsvc=svm.LinearSVC(C=10,max_iter=10000)        \n",
    "lsvc.fit(X.transpose(),y)                   #training / fitting the linear svm model     \n",
    "\n",
    "joblib.dump(lsvc,\"svm_classifier.sav\") # saving the trained model/ classifier(loaded in \"prediction.ipynb\" to predict the test labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
