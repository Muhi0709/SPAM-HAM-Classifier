{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "\n",
    "def symbol_removal(email):\n",
    "    e_mod=email\n",
    "    sym=[\"\\\\\",\"&\",\"!\",\".\",\",\",\"/\",\"+\",\"*\",\"%\",\"(\",\")\",\":\",\";\",\"\\\"\",\"_\",\"?\",\"=\",\"@\",\"{\",\"}\",\"[\",\"]\",\"#\"]\n",
    "    for s in sym:\n",
    "        e_mod=e_mod.replace(s,\" \")\n",
    "    e_mod.replace(\"\\'\",\"\")\n",
    "    e_mod=e_mod.replace(\"\\n\",\" \")\n",
    "    e_mod=e_mod.replace(\"-\",\"\")\n",
    "\n",
    "    return e_mod\n",
    "\n",
    "def lemmatisation(email,dic):\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        e_mod=email.replace(key,\" \"+dic[key]+\" \")\n",
    "    \n",
    "    return e_mod\n",
    "\n",
    "def num_token(email):\n",
    "    num=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    e_mod=email\n",
    "    for n in num:\n",
    "        e_mod=e_mod.replace(n,\" \"+\"0\"+\" \")\n",
    "    \n",
    "    return e_mod\n",
    "\n",
    "\n",
    "def idf(bag,lis):\n",
    "    idf_list=[]\n",
    "    N=len(lis)\n",
    "    for term in bag:\n",
    "        a=0\n",
    "        for doc in lis:\n",
    "            if term in doc:\n",
    "                a+=1\n",
    "        if a==0:\n",
    "            a=N\n",
    "        idf_list.append(a)\n",
    "    idf_vector=np.log(N/np.array(idf_list,dtype=\"float64\"))\n",
    "    return idf_vector\n",
    "\n",
    "def tf(bag,doc):\n",
    "    N=len(bag)\n",
    "    tf_list=[]\n",
    "    for i in range(N):\n",
    "        count=doc.count(bag[i])\n",
    "        tf_list.append(count/len(doc))\n",
    "    tf_vector=np.array(tf_list,dtype=\"float64\")\n",
    "    return tf_vector\n",
    "\n",
    "def tf_idf_vectorisation(bag,emails):\n",
    "    idf_v=idf(bag,emails)\n",
    "    N=len(bag)\n",
    "    num_of_docs=len(emails)\n",
    "    X=np.empty((N,num_of_docs))\n",
    "    for i in range(num_of_docs):\n",
    "        tf_v=tf(bag,emails[i])\n",
    "        X[:,i]=tf_v*idf_v\n",
    "    \n",
    "    return X\n",
    "\n",
    "def processing():           #function for processing the test emails\n",
    "    email=[]\n",
    "    directory=\"test\" \n",
    "    for file in sorted(os.listdir(directory),key= lambda x: int(x.replace(\"email\",\"\").replace(\".txt\",\"\"))):#reading in asc order\n",
    "        f=open(directory+\"/\"+file,\"r\",encoding='cp437')\n",
    "        b=f.readline().lower()\n",
    "        b=b.replace(\"subject: fw :\",\"\")\n",
    "        b=b.replace(\"subject: fw : fw :\",\"\")\n",
    "        b=b.replace(\"subject: re : re :\",\"\")        #removing the the word \"subject\" from emails at beginning of emails\n",
    "        b=b.replace(\"subject: re :\",\"\")\n",
    "        b=b.replace(\"subject: \",\"\")\n",
    "        a=b+f.read().lower()\n",
    "        email.append(a)\n",
    "        \n",
    "    lemma_dictionary=np.load(\"lemma_dictionary.npy\",allow_pickle=True).tolist()  #loading the lemma dictionary\n",
    "\n",
    "    for i in range(len(email)):\n",
    "        email[i]=symbol_removal(email[i])\n",
    "        email[i]=num_token(email[i])\n",
    "        email[i]=lemmatisation(email[i],lemma_dictionary)   #removing punctations, numbers and lemmatising the words\n",
    "\n",
    "    length=len(email)\n",
    "    email_bag=np.load(\"bagofwords.npy\").tolist()        #loading the bag_of_words \n",
    "\n",
    "    emails_list=[]\n",
    "    for i in range(length):\n",
    "        e=email[i].strip().split(\" \")\n",
    "        l=[x for x in e if x!=\" \" and x!=\"\" and x in email_bag]     # retaining only the words in the bag\n",
    "        if l!=[]:\n",
    "            emails_list.append(l)\n",
    "            \n",
    "    X_test=tf_idf_vectorisation(email_bag,emails_list)  #tf-idf vectorisation\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "def main():\n",
    "    \n",
    "    lsvc=joblib.load(\"svm_classifier.sav\") #loading the trained linear svm classifier(trained & saved via \"Training.ipynb\")\n",
    "    \n",
    "    X_test=processing().transpose()     #processing+ tf-idf vectorising of test emails\n",
    "    y_pred=lsvc.predict(X_test).astype(\"int32\")\n",
    "    print(y_pred)                     #printing the predicted labels\n",
    "    \n",
    "    return(y_pred)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=main()                        #invokes the main() func and label is returned as an array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
